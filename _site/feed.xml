<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Joe Heywood</title>
    <description>A blog about my experiences of kaggle competitions
</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 06 Feb 2016 22:58:22 +0000</pubDate>
    <lastBuildDate>Sat, 06 Feb 2016 22:58:22 +0000</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>whats cooking results</title>
        <description>&lt;p&gt;ok. so wrapping up.&lt;/p&gt;

&lt;p&gt;I feel I only just got started getting somewhere with this project about a week after the deadline. So I’ve made a decision to stop and move on. But below are a reasonably neat.&lt;/p&gt;

&lt;p&gt;I set up the data in a way that I can produce around 400 variables either as words in a simple bag of words model that takes out features that do not appear in at least 0.1% of recipes. This was my starting point and yielded a 73.0% success rate.&lt;/p&gt;

&lt;p&gt;I added another function to transform the text on three key problems that looked wrong.&lt;/p&gt;

&lt;p&gt;A third function takes all the words and condenses them into 400 variables using singular vector decomposition.&lt;/p&gt;

&lt;p&gt;This gives me three possible data frames to test out two different models: a multi-nominal logistic regression model using the &lt;code class=&quot;highlighter-rouge&quot;&gt;nnet&lt;/code&gt; package and a random forest model using &lt;code class=&quot;highlighter-rouge&quot;&gt;h2o&lt;/code&gt; package. The initial model is pretty reliable.&lt;/p&gt;

&lt;h3 id=&quot;text-transformation&quot;&gt;text transformation&lt;/h3&gt;

&lt;p&gt;Somewhat surprisingly, the text transformations make the model a little worse, rather than better. In previous testing, I had chosen N variables to be in the model. So a bad variable was taking the place of a potentially better one. But with the method of choosing variables based on the proportion of recipes it appears in removes this possibility. All it does is make the individual variables a little more intuitive for the person interpreting the coefficients without improving the performance of predicting on the testing data. The success rate dropped to 71.2% with this model.&lt;/p&gt;

&lt;h3 id=&quot;single-vector-decomposition-with-multinomial-logistic-regression&quot;&gt;single vector decomposition (with multinomial logistic regression&lt;/h3&gt;

&lt;p&gt;Using the SVD should work better for our logistic regression. Rather than clumsily messing with the data, as I had done before, it compresses the full data (including the very sparse terms) into a single matrix that is manageable for our regression model so contains more signal and less noise. It does improve our predictions a little, rather than being a substantial improvement. But an improvement of only 1% can make a big difference for gaggle competitions. This prediction took me up to 74.9%&lt;/p&gt;

&lt;h3 id=&quot;random-forest&quot;&gt;random forest&lt;/h3&gt;

&lt;p&gt;For random forest, as before, the initial data works quite well - though not quite as well as the logistic regression model - 72.9%. Applying the SVD model is much less effective. This makes sense, as the removal of that sparse data is presumably more geared towards regression model type algorithms than decision tree models. So while it doesn’t ruin the predictions - it does add some error.&lt;/p&gt;

&lt;p&gt;And there we go… next up: AirBnB&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Jan 2016 00:00:00 +0000</pubDate>
        <link>/2016/01/07/whats-cooking-results.html</link>
        <guid isPermaLink="true">/2016/01/07/whats-cooking-results.html</guid>
        
        
      </item>
    
      <item>
        <title>(whats cooking) predictions with logistic regression</title>
        <description>&lt;p&gt;I decided to start off with a simple model that I know well - logistic regression.
I’m sure that there are other, more effective ways of classifying our data, but
at this early stage, I decided to stick with what I know.&lt;/p&gt;

&lt;p&gt;In this case, we need to come up with a logit model that can handle several categories,
not just two. The twenty cuisines here are very much nominal categories, so they 
cannot be put into any kind of order, or given a numerical value that could 
correspond to the various cuisines.&lt;/p&gt;

&lt;p&gt;What we are looking for is a &lt;strong&gt;multinomial&lt;/strong&gt; logit model. Here, one category is removed 
and the coefficients for each variable (each word for us) corresponds to whether 
each possible category is more or less likely than the reference category. It 
works in a similar way to producing a separate logistic regression for each category
and comparing the predictions. The crucial difference is that the comparison is with a 
single category, rather than the entire sample.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;trainLogit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inTrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# create multinomial model. 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Args: dtm (data.frame), 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# inTrain(numeric vector) - for partition if needed - if not select 1:nrow(dtm)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# value multinomial logistic regression model (nnet::multinom)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;multinom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DV_cuisine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inTrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MaxNWts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;testLogit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# test multinomial logistic regression model
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Args: mod nnet:multinom object, 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# testData (data.frame) data on which to test model
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# should include column &#39;correct&#39; with correct cuisine category
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;probs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bestProb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Value data frame with nrow(testData) rows and columns:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# correct - the correct cuisine, prediction - the predicted cuisine
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# probs - the probability value for the best option and isCorr (bool) if prediction is correct
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DV_cuis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bestProb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;isCorr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DV_cuisine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inTrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;runLogit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# run model for a given seed (numeric). Get data from train.json,
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# create data partition from seed, create model on training partition, then test on test partition
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getTrainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;train.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;inTrain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getPartitionForSeed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;logitModel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainLogit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inTrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Value data.frame (from testLogit above)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;testLogit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logitModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inTrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;When we run the predict command on our testing data, we get the predicted value 
and when we run the same command, but with the &lt;code class=&quot;highlighter-rouge&quot;&gt;type = &quot;probs&quot;&lt;/code&gt; argument, we get a 
matrix of probablities of (number of cases) x (possible categories).&lt;/p&gt;

&lt;p&gt;The prediction, at this point matches the correct answer 69.9% of the time, 
which isn’t terrible, but equally it’s hardly going to set the kaggler competition
trembling,&lt;/p&gt;

&lt;p&gt;See below the frequency distribution of the probablities of the predictions. As 
you can see, the model is very accurate when the probability is high, then as you
get lower than around 50% it becomes little more than a guess. Even though this is
stating the obvious somewhat, I found this information to be useful as it does 
demonstrate that the probability value is a good indicator.&lt;/p&gt;

&lt;h4 id=&quot;frequency-distribution-of-probabilities&quot;&gt;Frequency distribution of probabilities&lt;/h4&gt;

&lt;!--html_preserve--&gt;
&lt;div id=&quot;plot_id459901946-container&quot; class=&quot;ggvis-output-container&quot;&gt;
&lt;div id=&quot;plot_id459901946&quot; class=&quot;ggvis-output&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;plot-gear-icon&quot;&gt;
&lt;nav class=&quot;ggvis-control&quot;&gt;
&lt;a class=&quot;ggvis-dropdown-toggle&quot; title=&quot;Controls&quot; onclick=&quot;return false;&quot;&gt;&lt;/a&gt;
&lt;ul class=&quot;ggvis-dropdown&quot;&gt;
&lt;li&gt;
Renderer: 
&lt;a id=&quot;plot_id459901946_renderer_svg&quot; class=&quot;ggvis-renderer-button&quot; onclick=&quot;return false;&quot; data-plot-id=&quot;plot_id459901946&quot; data-renderer=&quot;svg&quot;&gt;SVG&lt;/a&gt;
 | 
&lt;a id=&quot;plot_id459901946_renderer_canvas&quot; class=&quot;ggvis-renderer-button&quot; onclick=&quot;return false;&quot; data-plot-id=&quot;plot_id459901946&quot; data-renderer=&quot;canvas&quot;&gt;Canvas&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a id=&quot;plot_id459901946_download&quot; class=&quot;ggvis-download&quot; data-plot-id=&quot;plot_id459901946&quot;&gt;Download&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
var plot_id459901946_spec = {
  &quot;data&quot;: [
    {
      &quot;name&quot;: &quot;.0/bin1_flat&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {
          &quot;x_&quot;: &quot;number&quot;,
          &quot;count_&quot;: &quot;number&quot;
        }
      },
      &quot;values&quot;: &quot;\&quot;isCorr\&quot;,\&quot;x_\&quot;,\&quot;count_\&quot;\n\&quot;Incorrect\&quot;,0.05,0\n\&quot;Incorrect\&quot;,0.1,1\n\&quot;Incorrect\&quot;,0.15,23\n\&quot;Incorrect\&quot;,0.2,97\n\&quot;Incorrect\&quot;,0.25,137\n\&quot;Incorrect\&quot;,0.3,193\n\&quot;Incorrect\&quot;,0.35,192\n\&quot;Incorrect\&quot;,0.4,174\n\&quot;Incorrect\&quot;,0.45,170\n\&quot;Incorrect\&quot;,0.5,183\n\&quot;Incorrect\&quot;,0.55,168\n\&quot;Incorrect\&quot;,0.6,122\n\&quot;Incorrect\&quot;,0.65,117\n\&quot;Incorrect\&quot;,0.7,102\n\&quot;Incorrect\&quot;,0.75,72\n\&quot;Incorrect\&quot;,0.8,79\n\&quot;Incorrect\&quot;,0.85,88\n\&quot;Incorrect\&quot;,0.9,70\n\&quot;Incorrect\&quot;,0.95,80\n\&quot;Incorrect\&quot;,1,46\n\&quot;Incorrect\&quot;,1.05,0\n\&quot;Correct\&quot;,0.1,0\n\&quot;Correct\&quot;,0.15,5\n\&quot;Correct\&quot;,0.2,20\n\&quot;Correct\&quot;,0.25,63\n\&quot;Correct\&quot;,0.3,77\n\&quot;Correct\&quot;,0.35,103\n\&quot;Correct\&quot;,0.4,126\n\&quot;Correct\&quot;,0.45,146\n\&quot;Correct\&quot;,0.5,196\n\&quot;Correct\&quot;,0.55,177\n\&quot;Correct\&quot;,0.6,209\n\&quot;Correct\&quot;,0.65,195\n\&quot;Correct\&quot;,0.7,231\n\&quot;Correct\&quot;,0.75,258\n\&quot;Correct\&quot;,0.8,317\n\&quot;Correct\&quot;,0.85,322\n\&quot;Correct\&quot;,0.9,496\n\&quot;Correct\&quot;,0.95,869\n\&quot;Correct\&quot;,1,2023\n\&quot;Correct\&quot;,1.05,0&quot;
    },
    {
      &quot;name&quot;: &quot;.0/bin1&quot;,
      &quot;source&quot;: &quot;.0/bin1_flat&quot;,
      &quot;transform&quot;: [
        {
          &quot;type&quot;: &quot;treefacet&quot;,
          &quot;keys&quot;: [
            &quot;data.isCorr&quot;
          ]
        }
      ]
    },
    {
      &quot;name&quot;: &quot;scale/stroke&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {}
      },
      &quot;values&quot;: &quot;\&quot;domain\&quot;\n\&quot;Incorrect\&quot;\n\&quot;Correct\&quot;&quot;
    },
    {
      &quot;name&quot;: &quot;scale/x&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {
          &quot;domain&quot;: &quot;number&quot;
        }
      },
      &quot;values&quot;: &quot;\&quot;domain\&quot;\n-6.93889390390723e-18\n1.1&quot;
    },
    {
      &quot;name&quot;: &quot;scale/x_rel&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {
          &quot;domain&quot;: &quot;number&quot;
        }
      },
      &quot;values&quot;: &quot;\&quot;domain\&quot;\n0\n1&quot;
    },
    {
      &quot;name&quot;: &quot;scale/y&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {
          &quot;domain&quot;: &quot;number&quot;
        }
      },
      &quot;values&quot;: &quot;\&quot;domain\&quot;\n-101.15\n2124.15&quot;
    },
    {
      &quot;name&quot;: &quot;scale/y_rel&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {
          &quot;domain&quot;: &quot;number&quot;
        }
      },
      &quot;values&quot;: &quot;\&quot;domain\&quot;\n0\n1&quot;
    }
  ],
  &quot;scales&quot;: [
    {
      &quot;name&quot;: &quot;stroke&quot;,
      &quot;type&quot;: &quot;ordinal&quot;,
      &quot;domain&quot;: {
        &quot;data&quot;: &quot;scale/stroke&quot;,
        &quot;field&quot;: &quot;data.domain&quot;
      },
      &quot;points&quot;: true,
      &quot;sort&quot;: false,
      &quot;range&quot;: &quot;category10&quot;
    },
    {
      &quot;name&quot;: &quot;x&quot;,
      &quot;domain&quot;: {
        &quot;data&quot;: &quot;scale/x&quot;,
        &quot;field&quot;: &quot;data.domain&quot;
      },
      &quot;zero&quot;: false,
      &quot;nice&quot;: false,
      &quot;clamp&quot;: false,
      &quot;range&quot;: &quot;width&quot;
    },
    {
      &quot;name&quot;: &quot;x_rel&quot;,
      &quot;domain&quot;: {
        &quot;data&quot;: &quot;scale/x_rel&quot;,
        &quot;field&quot;: &quot;data.domain&quot;
      },
      &quot;range&quot;: &quot;width&quot;,
      &quot;zero&quot;: false,
      &quot;nice&quot;: false,
      &quot;clamp&quot;: false
    },
    {
      &quot;name&quot;: &quot;y&quot;,
      &quot;domain&quot;: {
        &quot;data&quot;: &quot;scale/y&quot;,
        &quot;field&quot;: &quot;data.domain&quot;
      },
      &quot;zero&quot;: false,
      &quot;nice&quot;: false,
      &quot;clamp&quot;: false,
      &quot;range&quot;: &quot;height&quot;
    },
    {
      &quot;name&quot;: &quot;y_rel&quot;,
      &quot;domain&quot;: {
        &quot;data&quot;: &quot;scale/y_rel&quot;,
        &quot;field&quot;: &quot;data.domain&quot;
      },
      &quot;range&quot;: &quot;height&quot;,
      &quot;zero&quot;: false,
      &quot;nice&quot;: false,
      &quot;clamp&quot;: false
    }
  ],
  &quot;marks&quot;: [
    {
      &quot;type&quot;: &quot;group&quot;,
      &quot;from&quot;: {
        &quot;data&quot;: &quot;.0/bin1&quot;
      },
      &quot;marks&quot;: [
        {
          &quot;type&quot;: &quot;line&quot;,
          &quot;properties&quot;: {
            &quot;update&quot;: {
              &quot;stroke&quot;: {
                &quot;scale&quot;: &quot;stroke&quot;,
                &quot;field&quot;: &quot;data.isCorr&quot;
              },
              &quot;strokeWidth&quot;: {
                &quot;value&quot;: 5
              },
              &quot;x&quot;: {
                &quot;scale&quot;: &quot;x&quot;,
                &quot;field&quot;: &quot;data.x_&quot;
              },
              &quot;y&quot;: {
                &quot;scale&quot;: &quot;y&quot;,
                &quot;field&quot;: &quot;data.count_&quot;
              }
            },
            &quot;ggvis&quot;: {
              &quot;data&quot;: {
                &quot;value&quot;: &quot;.0/bin1&quot;
              }
            }
          }
        }
      ]
    }
  ],
  &quot;legends&quot;: [
    {
      &quot;orient&quot;: &quot;right&quot;,
      &quot;title&quot;: &quot;&quot;,
      &quot;properties&quot;: {
        &quot;legend&quot;: {
          &quot;x&quot;: {
            &quot;scale&quot;: &quot;x_rel&quot;,
            &quot;value&quot;: 0.1
          },
          &quot;y&quot;: {
            &quot;scale&quot;: &quot;y_rel&quot;,
            &quot;value&quot;: 0.9
          }
        }
      },
      &quot;stroke&quot;: &quot;stroke&quot;
    }
  ],
  &quot;axes&quot;: [
    {
      &quot;type&quot;: &quot;x&quot;,
      &quot;scale&quot;: &quot;x&quot;,
      &quot;orient&quot;: &quot;bottom&quot;,
      &quot;title&quot;: &quot;Probability value&quot;,
      &quot;format&quot;: &quot;%&quot;,
      &quot;values&quot;: [0.2, 0.4, 0.6, 0.8, 1],
      &quot;layer&quot;: &quot;back&quot;,
      &quot;grid&quot;: true,
      &quot;properties&quot;: {
        &quot;grid&quot;: {
          &quot;stroke&quot;: {
            &quot;value&quot;: &quot;white&quot;
          }
        }
      }
    },
    {
      &quot;type&quot;: &quot;y&quot;,
      &quot;scale&quot;: &quot;y&quot;,
      &quot;orient&quot;: &quot;left&quot;,
      &quot;title&quot;: &quot;&quot;,
      &quot;values&quot;: [500, 1000, 1500, 2000],
      &quot;layer&quot;: &quot;back&quot;,
      &quot;grid&quot;: true,
      &quot;properties&quot;: {
        &quot;grid&quot;: {
          &quot;stroke&quot;: {
            &quot;value&quot;: &quot;white&quot;
          }
        }
      }
    }
  ],
  &quot;padding&quot;: null,
  &quot;ggvis_opts&quot;: {
    &quot;keep_aspect&quot;: false,
    &quot;resizable&quot;: true,
    &quot;padding&quot;: {},
    &quot;duration&quot;: 250,
    &quot;renderer&quot;: &quot;svg&quot;,
    &quot;hover_duration&quot;: 0,
    &quot;width&quot;: &quot;700px&quot;,
    &quot;height&quot;: &quot;480px&quot;
  },
  &quot;handlers&quot;: null
};
ggvis.getPlot(&quot;plot_id459901946&quot;).parseSpec(plot_id459901946_spec);
&lt;/script&gt;
&lt;!--/html_preserve--&gt;

&lt;h2 id=&quot;text-transformations&quot;&gt;Text transformations&lt;/h2&gt;

&lt;p&gt;My first thoughts to improve this model were to take out some of the less helpful
terms in the bag of words. I didn’t do a great deal of this, just a few things 
that looked wrong to me. First - there were a lot of adjectives in the data set
that did not seem to be helpful to my model, but were quite common, so were part of
my model. Examples of this include “fresh”, “firm”, “frozen” etc. A second category
is two separate words that together means something, but apart, just as two features 
in my bag of words did nothing to help my model. For example, “fish sauce” is a 
dead giveaway for Thai food. But in the bag of words, they appeared as “fish” and
“sauce”, which could have been part of any cuisine. The rest were lots of different
words for the same thing. In particular, the difference between ground pepper, chili
peppers and bell peppers needed to be normalized. “red pepper”, “pepper”, “red chiles”
“chili peppers” needed sorting out.&lt;/p&gt;

&lt;p&gt;I stopped there and wondered if the more I did was starting to hinder my models
as much as they were helping. But I basically came up with five functions that 
were a quick transformation to the text using a regex of some kind to either 
normalize a particular ingredient, remove unnecessary adjectives or join words 
together. In this end, this didn’t help my predicitions at all. The way that 
I was choosing the variables in my ‘bag of words’ model was removing the sparse
terms (so including words that appear on more than x% of recipes).&lt;/p&gt;

&lt;h2 id=&quot;feature-selection&quot;&gt;Feature selection&lt;/h2&gt;

&lt;p&gt;My previous experience working with regression models gave me a nagging sense that
I ought to be doing something more effective in selecting features. In my previous
research, this was done by reading lots of theory and other empirical research to 
carefully select the variables in my model. This was not possible with such a large
data set (and last time I checked there wasn’t really a body of research on this 
which ingredients belong to which cuisine!) I saw advice on an online
forum to try some kind of algorithm for selecting features that might make my
model a bit more efficient. I tried on called a singular vector decomposition. 
It took me a while to get my head around how this algorithm works - but basically 
it removes redundant data from your larger and sparser matrix into a smaller and 
denser one. Data might be redundant if variables too closely correlate with one 
another or don’t really give much data to indicate anything about the dependent 
variable - so act as noise. As I understand it, this makes your model a bit more 
efficient and a lot smaller. But don’t take my word for it - a brief explanation is
here and a longer one is here.&lt;/p&gt;

&lt;p&gt;It takes quite a while to reduce the matrix - I used what is known as a partial 
SVD using the &lt;code class=&quot;highlighter-rouge&quot;&gt;irlba&lt;/code&gt; package to do this. This package uses the SVD algorithm to
reduce the original matrix to a smaller size and returns a vector that we can
multiply through the original matrix to get it to the right size.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;getIrlba&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;all.data.svd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;irlba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;as.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all.data.svd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The updated results? A small improvement. Not massive. But an improvement all the
same.&lt;/p&gt;

</description>
        <pubDate>Wed, 23 Dec 2015 00:00:00 +0000</pubDate>
        <link>/2015/12/23/whats-cooking-(2-of-3)-predictions-with-logistic-regression.html</link>
        <guid isPermaLink="true">/2015/12/23/whats-cooking-(2-of-3)-predictions-with-logistic-regression.html</guid>
        
        
      </item>
    
      <item>
        <title>whats cooking intro</title>
        <description>&lt;p&gt;The first competition I entered came under the title “What’s cooking” and involved predicting which part of the world recipes, that appeared on yummly.com, came from. I liked the sound of this competition, as I am a bit of a foodie - so it seemed like a good place to start.&lt;/p&gt;

&lt;p&gt;The data came in JSON format, giving an ID number, a list of ingredients and one of twenty cuisines. My principle goal for this competiton was to come up with a plan and execute it, hopefully coming up with a template for future projects, some reusable code and a few lessons learned. More important than getting a high score was understanding how and why a model might work and perhaps its limitations. In this post, I briefly go over how to get the data, get it ready to be analyzed and some simple analysis.&lt;/p&gt;

&lt;h3 id=&quot;getting-the-data&quot;&gt;Getting the data&lt;/h3&gt;

&lt;p&gt;Converting the data to a data.frame from JSON was pretty simple, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;jsonlite&lt;/code&gt; package. This flattens the hierarchical json file into one that is easier to work with in R.&lt;/p&gt;

&lt;p&gt;I then start using the &lt;code class=&quot;highlighter-rouge&quot;&gt;tm&lt;/code&gt; package to put each item of ingredients into a Corpus, so we can start making transformations to the text and then create a Document Term Matrix that can be used in our various models. This matrix is essentially what is known as a ‘bag of words’, where each word in the recipe - regardless of what came before and after it.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;getTrainingData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jsonFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# get data from json file and convert to data frame
&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Arg: the path of the json file
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fromJSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simplifyDataFrame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ingredients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VectorSource&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ingredients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# future transformations to the ingredients will go here...
&lt;/span&gt;  
  &lt;span class=&quot;c1&quot;&gt;#  convert to document term matrix and coerce to data.frame
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DocumentTermMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ingredients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# add dependent variable (cuisine) from train object
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DV_cuisine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuisine&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# Value:  a data frame with DV_cuisine for the cuisine and a column for each word in the 
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# data.frame object returned
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;err&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getTrainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;train.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;distribution-of-cuisines&quot;&gt;Distribution of cuisines&lt;/h3&gt;

&lt;p&gt;There are twenty cuisines, as you can see below, the cuisines are not equally distributed. There are more Western cuisines than Eastern and quite a big proportion are Italian, Mexican, Indian, Southern US and Chinese.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/figure/treemap-1.png&quot; alt=&quot;plot of chunk treemap&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Below is a quick word cloud of the most common terms found in the ingredients lists. 
This gives us a bit of insight into how the data works, in our ‘bag of words’ structure
some of the words there don’t seem to make much sense without the preceding word 
(such as paste, powder, leaves etc) and others seem to be redundant adjectives (fresh,
large, chopped) which probably don’t tell us much about which cuisine the recipe might
belong to. I had a go at cleaning this data - and will explain hot this panned out in the next post.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;frq.Rda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;frq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wordcloud&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/figure/wordcloud-1.png&quot; alt=&quot;plot of chunk wordcloud&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;setting-up-a-partition&quot;&gt;Setting up a partition&lt;/h2&gt;

&lt;p&gt;In order to test our models, we need to separate our training data into a further
training and testing set. I did this using the caret package. This allows us to test our algorithms on a number of different training sets, altering the proportion of the 
training/testing sets if we wish.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;caret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;getPartitionForSeed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;set.seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;createDataPartition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DV_cuisine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Having done all of that, we’re ready to start coming up with a prediction model.&lt;/p&gt;

</description>
        <pubDate>Tue, 22 Dec 2015 00:00:00 +0000</pubDate>
        <link>/2015/12/22/whats-cooking-intro.html</link>
        <guid isPermaLink="true">/2015/12/22/whats-cooking-intro.html</guid>
        
        
      </item>
    
  </channel>
</rss>
