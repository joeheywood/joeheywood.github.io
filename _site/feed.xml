<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Joe Heywood</title>
    <description>A blog about my experiences of kaggle competitions
</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 16 Jan 2016 12:48:51 +0000</pubDate>
    <lastBuildDate>Sat, 16 Jan 2016 12:48:51 +0000</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>predictions with logistic regression</title>
        <description>&lt;p&gt;I decided to start off with a simple model that I know well - logistic regression.
I’m sure that there are other, more effective ways of classifying our data, but
at this early stage, I decided to stick with what I know.&lt;/p&gt;

&lt;p&gt;In this case, we need to come up with a logit model that can handle several categories,
not just two. The twenty cuisines here are very much nominal categories, so they 
cannot be put into any kind of order, or given a numerical value that could 
correspond to the various cuisines.&lt;/p&gt;

&lt;p&gt;What we are looking for is a &lt;strong&gt;multinomial&lt;/strong&gt; logit model. Here, one category is removed 
and the coefficients for each variable (each word for us) corresponds to whether 
each possible category is more or less likely than the reference category. It 
works in a similar way to producing a separate logistic regression for each category
and comparing the predictions. The crucial difference is that the comparison is with a 
single category, rather than the entire sample.&lt;/p&gt;

&lt;p&gt;```r
trainLogit &amp;lt;- function(dtm, inTrain) {
    multinom(DV_cuisine ~ ., data = dtm[inTrain,], MaxNWts = 10000)
}&lt;/p&gt;

&lt;p&gt;testLogit &amp;lt;- function(mod, testData) {
    preds &amp;lt;- predict(mod, testData)
    probs &amp;lt;- predict(mod, testData, type = “probs”)
    bestProb &amp;lt;- sapply(1:length(preds), function(x) {
        probs[x, preds[x]]
    })
    data.frame(correct = testData$DV_cuis, prediction = preds,
               probs = bestProb, isCorr = dtm$DV_cuisine[-inTrain] == preds)
}&lt;/p&gt;

&lt;p&gt;runLogit &amp;lt;- function(seed) {
    dtm &amp;lt;- getTrainingData(“train.json”)
    inTrain &amp;lt;- getPartitionForSeed(seed, dtm)
    logitModel &amp;lt;- trainLogit(dtm, inTrain)
    results &amp;lt;- testLogit(logitModel, dtm[-inTrain])
}
```&lt;/p&gt;

&lt;p&gt;When we run the predict command on our testing data, we get the predicted value 
and when we run the same command, but with the &lt;code class=&quot;highlighter-rouge&quot;&gt;type = &quot;probs&quot;&lt;/code&gt; argument, we get a 
matrix of probablities of (number of cases) x (possible categories).&lt;/p&gt;

&lt;p&gt;The prediction, at this point matches the correct answer 69.9% of the time, 
which isn’t terrible, but equally it’s hardly going to set the kaggler competition
trembling,&lt;/p&gt;

&lt;p&gt;See below the frequency distribution of the probablities of the predictions. As 
you can see, the model is very accurate when the probability is high, then as you
get lower than around 50% it becomes little more than a guess. Even though this is
stating the obvious somewhat, I found this information to be useful as it does 
demonstrate that the probability value is a good indicator.&lt;/p&gt;

&lt;h4 id=&quot;frequency-distribution-of-probabilities&quot;&gt;Frequency distribution of probabilities&lt;/h4&gt;

&lt;!--html_preserve--&gt;
&lt;div id=&quot;plot_id459901946-container&quot; class=&quot;ggvis-output-container&quot;&gt;
&lt;div id=&quot;plot_id459901946&quot; class=&quot;ggvis-output&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;plot-gear-icon&quot;&gt;
&lt;nav class=&quot;ggvis-control&quot;&gt;
&lt;a class=&quot;ggvis-dropdown-toggle&quot; title=&quot;Controls&quot; onclick=&quot;return false;&quot;&gt;&lt;/a&gt;
&lt;ul class=&quot;ggvis-dropdown&quot;&gt;
&lt;li&gt;
Renderer: 
&lt;a id=&quot;plot_id459901946_renderer_svg&quot; class=&quot;ggvis-renderer-button&quot; onclick=&quot;return false;&quot; data-plot-id=&quot;plot_id459901946&quot; data-renderer=&quot;svg&quot;&gt;SVG&lt;/a&gt;
 | 
&lt;a id=&quot;plot_id459901946_renderer_canvas&quot; class=&quot;ggvis-renderer-button&quot; onclick=&quot;return false;&quot; data-plot-id=&quot;plot_id459901946&quot; data-renderer=&quot;canvas&quot;&gt;Canvas&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a id=&quot;plot_id459901946_download&quot; class=&quot;ggvis-download&quot; data-plot-id=&quot;plot_id459901946&quot;&gt;Download&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
var plot_id459901946_spec = {
  &quot;data&quot;: [
    {
      &quot;name&quot;: &quot;.0/bin1_flat&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {
          &quot;x_&quot;: &quot;number&quot;,
          &quot;count_&quot;: &quot;number&quot;
        }
      },
      &quot;values&quot;: &quot;\&quot;isCorr\&quot;,\&quot;x_\&quot;,\&quot;count_\&quot;\n\&quot;Incorrect\&quot;,0.05,0\n\&quot;Incorrect\&quot;,0.1,1\n\&quot;Incorrect\&quot;,0.15,23\n\&quot;Incorrect\&quot;,0.2,97\n\&quot;Incorrect\&quot;,0.25,137\n\&quot;Incorrect\&quot;,0.3,193\n\&quot;Incorrect\&quot;,0.35,192\n\&quot;Incorrect\&quot;,0.4,174\n\&quot;Incorrect\&quot;,0.45,170\n\&quot;Incorrect\&quot;,0.5,183\n\&quot;Incorrect\&quot;,0.55,168\n\&quot;Incorrect\&quot;,0.6,122\n\&quot;Incorrect\&quot;,0.65,117\n\&quot;Incorrect\&quot;,0.7,102\n\&quot;Incorrect\&quot;,0.75,72\n\&quot;Incorrect\&quot;,0.8,79\n\&quot;Incorrect\&quot;,0.85,88\n\&quot;Incorrect\&quot;,0.9,70\n\&quot;Incorrect\&quot;,0.95,80\n\&quot;Incorrect\&quot;,1,46\n\&quot;Incorrect\&quot;,1.05,0\n\&quot;Correct\&quot;,0.1,0\n\&quot;Correct\&quot;,0.15,5\n\&quot;Correct\&quot;,0.2,20\n\&quot;Correct\&quot;,0.25,63\n\&quot;Correct\&quot;,0.3,77\n\&quot;Correct\&quot;,0.35,103\n\&quot;Correct\&quot;,0.4,126\n\&quot;Correct\&quot;,0.45,146\n\&quot;Correct\&quot;,0.5,196\n\&quot;Correct\&quot;,0.55,177\n\&quot;Correct\&quot;,0.6,209\n\&quot;Correct\&quot;,0.65,195\n\&quot;Correct\&quot;,0.7,231\n\&quot;Correct\&quot;,0.75,258\n\&quot;Correct\&quot;,0.8,317\n\&quot;Correct\&quot;,0.85,322\n\&quot;Correct\&quot;,0.9,496\n\&quot;Correct\&quot;,0.95,869\n\&quot;Correct\&quot;,1,2023\n\&quot;Correct\&quot;,1.05,0&quot;
    },
    {
      &quot;name&quot;: &quot;.0/bin1&quot;,
      &quot;source&quot;: &quot;.0/bin1_flat&quot;,
      &quot;transform&quot;: [
        {
          &quot;type&quot;: &quot;treefacet&quot;,
          &quot;keys&quot;: [
            &quot;data.isCorr&quot;
          ]
        }
      ]
    },
    {
      &quot;name&quot;: &quot;scale/stroke&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {}
      },
      &quot;values&quot;: &quot;\&quot;domain\&quot;\n\&quot;Incorrect\&quot;\n\&quot;Correct\&quot;&quot;
    },
    {
      &quot;name&quot;: &quot;scale/x&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {
          &quot;domain&quot;: &quot;number&quot;
        }
      },
      &quot;values&quot;: &quot;\&quot;domain\&quot;\n-6.93889390390723e-18\n1.1&quot;
    },
    {
      &quot;name&quot;: &quot;scale/x_rel&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {
          &quot;domain&quot;: &quot;number&quot;
        }
      },
      &quot;values&quot;: &quot;\&quot;domain\&quot;\n0\n1&quot;
    },
    {
      &quot;name&quot;: &quot;scale/y&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {
          &quot;domain&quot;: &quot;number&quot;
        }
      },
      &quot;values&quot;: &quot;\&quot;domain\&quot;\n-101.15\n2124.15&quot;
    },
    {
      &quot;name&quot;: &quot;scale/y_rel&quot;,
      &quot;format&quot;: {
        &quot;type&quot;: &quot;csv&quot;,
        &quot;parse&quot;: {
          &quot;domain&quot;: &quot;number&quot;
        }
      },
      &quot;values&quot;: &quot;\&quot;domain\&quot;\n0\n1&quot;
    }
  ],
  &quot;scales&quot;: [
    {
      &quot;name&quot;: &quot;stroke&quot;,
      &quot;type&quot;: &quot;ordinal&quot;,
      &quot;domain&quot;: {
        &quot;data&quot;: &quot;scale/stroke&quot;,
        &quot;field&quot;: &quot;data.domain&quot;
      },
      &quot;points&quot;: true,
      &quot;sort&quot;: false,
      &quot;range&quot;: &quot;category10&quot;
    },
    {
      &quot;name&quot;: &quot;x&quot;,
      &quot;domain&quot;: {
        &quot;data&quot;: &quot;scale/x&quot;,
        &quot;field&quot;: &quot;data.domain&quot;
      },
      &quot;zero&quot;: false,
      &quot;nice&quot;: false,
      &quot;clamp&quot;: false,
      &quot;range&quot;: &quot;width&quot;
    },
    {
      &quot;name&quot;: &quot;x_rel&quot;,
      &quot;domain&quot;: {
        &quot;data&quot;: &quot;scale/x_rel&quot;,
        &quot;field&quot;: &quot;data.domain&quot;
      },
      &quot;range&quot;: &quot;width&quot;,
      &quot;zero&quot;: false,
      &quot;nice&quot;: false,
      &quot;clamp&quot;: false
    },
    {
      &quot;name&quot;: &quot;y&quot;,
      &quot;domain&quot;: {
        &quot;data&quot;: &quot;scale/y&quot;,
        &quot;field&quot;: &quot;data.domain&quot;
      },
      &quot;zero&quot;: false,
      &quot;nice&quot;: false,
      &quot;clamp&quot;: false,
      &quot;range&quot;: &quot;height&quot;
    },
    {
      &quot;name&quot;: &quot;y_rel&quot;,
      &quot;domain&quot;: {
        &quot;data&quot;: &quot;scale/y_rel&quot;,
        &quot;field&quot;: &quot;data.domain&quot;
      },
      &quot;range&quot;: &quot;height&quot;,
      &quot;zero&quot;: false,
      &quot;nice&quot;: false,
      &quot;clamp&quot;: false
    }
  ],
  &quot;marks&quot;: [
    {
      &quot;type&quot;: &quot;group&quot;,
      &quot;from&quot;: {
        &quot;data&quot;: &quot;.0/bin1&quot;
      },
      &quot;marks&quot;: [
        {
          &quot;type&quot;: &quot;line&quot;,
          &quot;properties&quot;: {
            &quot;update&quot;: {
              &quot;stroke&quot;: {
                &quot;scale&quot;: &quot;stroke&quot;,
                &quot;field&quot;: &quot;data.isCorr&quot;
              },
              &quot;strokeWidth&quot;: {
                &quot;value&quot;: 5
              },
              &quot;x&quot;: {
                &quot;scale&quot;: &quot;x&quot;,
                &quot;field&quot;: &quot;data.x_&quot;
              },
              &quot;y&quot;: {
                &quot;scale&quot;: &quot;y&quot;,
                &quot;field&quot;: &quot;data.count_&quot;
              }
            },
            &quot;ggvis&quot;: {
              &quot;data&quot;: {
                &quot;value&quot;: &quot;.0/bin1&quot;
              }
            }
          }
        }
      ]
    }
  ],
  &quot;legends&quot;: [
    {
      &quot;orient&quot;: &quot;right&quot;,
      &quot;title&quot;: &quot;&quot;,
      &quot;properties&quot;: {
        &quot;legend&quot;: {
          &quot;x&quot;: {
            &quot;scale&quot;: &quot;x_rel&quot;,
            &quot;value&quot;: 0.1
          },
          &quot;y&quot;: {
            &quot;scale&quot;: &quot;y_rel&quot;,
            &quot;value&quot;: 0.9
          }
        }
      },
      &quot;stroke&quot;: &quot;stroke&quot;
    }
  ],
  &quot;axes&quot;: [
    {
      &quot;type&quot;: &quot;x&quot;,
      &quot;scale&quot;: &quot;x&quot;,
      &quot;orient&quot;: &quot;bottom&quot;,
      &quot;title&quot;: &quot;Probability value&quot;,
      &quot;format&quot;: &quot;%&quot;,
      &quot;values&quot;: [0.2, 0.4, 0.6, 0.8, 1],
      &quot;layer&quot;: &quot;back&quot;,
      &quot;grid&quot;: true,
      &quot;properties&quot;: {
        &quot;grid&quot;: {
          &quot;stroke&quot;: {
            &quot;value&quot;: &quot;white&quot;
          }
        }
      }
    },
    {
      &quot;type&quot;: &quot;y&quot;,
      &quot;scale&quot;: &quot;y&quot;,
      &quot;orient&quot;: &quot;left&quot;,
      &quot;title&quot;: &quot;&quot;,
      &quot;values&quot;: [500, 1000, 1500, 2000],
      &quot;layer&quot;: &quot;back&quot;,
      &quot;grid&quot;: true,
      &quot;properties&quot;: {
        &quot;grid&quot;: {
          &quot;stroke&quot;: {
            &quot;value&quot;: &quot;white&quot;
          }
        }
      }
    }
  ],
  &quot;padding&quot;: null,
  &quot;ggvis_opts&quot;: {
    &quot;keep_aspect&quot;: false,
    &quot;resizable&quot;: true,
    &quot;padding&quot;: {},
    &quot;duration&quot;: 250,
    &quot;renderer&quot;: &quot;svg&quot;,
    &quot;hover_duration&quot;: 0,
    &quot;width&quot;: &quot;700px&quot;,
    &quot;height&quot;: &quot;480px&quot;
  },
  &quot;handlers&quot;: null
};
ggvis.getPlot(&quot;plot_id459901946&quot;).parseSpec(plot_id459901946_spec);
&lt;/script&gt;
&lt;!--/html_preserve--&gt;

&lt;h2 id=&quot;text-transformations&quot;&gt;Text transformations&lt;/h2&gt;

&lt;p&gt;My first thoughts to improve this model were to take out some of the less helpful
terms in the bag of words. I didn’t do a great deal of this, just a few things 
that looked wrong to me. First - there were a lot of adjectives in the data set
that did not seem to be helpful to my model, but were quite common, so were part of
my model. Examples of this include “fresh”, “firm”, “frozen” etc. A second category
is two separate words that together means something, but apart, just as two features 
in my bag of words did nothing to help my model. For example, “fish sauce” is a 
dead giveaway for Thai food. But in the bag of words, they appeared as “fish” and
“sauce”, which could have been part of any cuisine. The rest were lots of different
words for the same thing. In particular, the difference between ground pepper, chili
peppers and bell peppers needed to be normalized. “red pepper”, “pepper”, “red chiles”
“chili peppers” needed sorting out.&lt;/p&gt;

&lt;p&gt;I stopped there and wondered if the more I did was starting to hinder my models
as much as they were helping. But I basically came up with five functions that 
were a quick transformation to the text using a regex of some kind to either 
normalize a particular ingredient, remove unnecessary adjectives or join words 
together. In this end, this didn’t help my predicitions at all. The way that 
I was choosing the variables in my ‘bag of words’ model was removing the sparse
terms (so including words that appear on more than x% of recipes).&lt;/p&gt;

&lt;h2 id=&quot;feature-selection&quot;&gt;Feature selection&lt;/h2&gt;

&lt;p&gt;My previous experience working with regression models gave me a nagging sense that
I ought to be doing something more effective in selecting features. In my previous
research, this was done by reading lots of theory and other empirical research to 
carefully select the variables in my model. This was not possible with such a large
data set (and last time I checked there wasn’t really a body of research on this 
which ingredients belong to which cuisine!) I saw advice on an online
forum to try some kind of algorithm for selecting features that might make my
model a bit more efficient. I tried on called a singular vector decomposition. 
It took me a while to get my head around how this algorithm works - but basically 
it removes redundant data from your larger and sparser matrix into a smaller and 
denser one. Data might be redundant if variables too closely correlate with one 
another or don’t really give much data to indicate anything about the dependent 
variable - so act as noise. As I understand it, this makes your model a bit more 
efficient and a lot smaller. But don’t take my word for it - a brief explanation is
here and a longer one is here.&lt;/p&gt;

&lt;p&gt;It takes quite a while to reduce the matrix - I used what is known as a partial 
SVD using the &lt;code class=&quot;highlighter-rouge&quot;&gt;irlba&lt;/code&gt; package to do this. This package uses the SVD algorithm to
reduce the original matrix to a smaller size and returns a vector that we can
multiply through the original matrix to get it to the right size.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;r
getIrlba &amp;lt;- function(dtm) {
    all.data.svd = irlba(dtm, nv = 400,nu=0)
    as.data.frame(dtm %*% all.data.svd$v)
}
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The updated results? A small improvement. Not massive. But an improvement all the
same.&lt;/p&gt;

</description>
        <pubDate>Wed, 23 Dec 2015 00:00:00 +0000</pubDate>
        <link>/2015/12/23/predictions-with-logistic-regression.html</link>
        <guid isPermaLink="true">/2015/12/23/predictions-with-logistic-regression.html</guid>
        
        
      </item>
    
      <item>
        <title>Welcome to Jekyll!</title>
        <description>&lt;p&gt;You’ll find this post in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;Tom&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &#39;Hi, Tom&#39; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Tue, 22 Dec 2015 22:56:20 +0000</pubDate>
        <link>/jekyll/update/2015/12/22/welcome-to-jekyll.html</link>
        <guid isPermaLink="true">/jekyll/update/2015/12/22/welcome-to-jekyll.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>whats cooking intro</title>
        <description>&lt;p&gt;The first competition I entered came under the title “What’s cooking” and involved
coming up with predictions about what cuisine (mostly nationality) a particular 
recipe (that appeared on yummly.com) came from. I liked the sound of this 
competition, as I am a bit of a foodie and I am very interested in learning how 
to process text in R, rather than in python.&lt;/p&gt;

&lt;p&gt;The training and testing data came in JSON format, giving an ID number, a list 
of ingredients and (in the training data only) one of twenty cuisines. Having 
worked a bit with Naive Bayes classifiers, I immediately thought that this was
going to be difficult to categorize in a single model.&lt;/p&gt;

&lt;p&gt;My principle goal for this model was to come up with a plan and execute it, 
hopefully coming up with a template for future projects, some reusable code and
learn a few lessons. In this post, I briefly go over how to get the data, get it ready
to be analyzed and some simple analysis.&lt;/p&gt;

&lt;h3 id=&quot;getting-the-data&quot;&gt;Getting the data&lt;/h3&gt;

&lt;p&gt;Converting the data to a data.frame from JSON is pretty simple, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;jsonlite&lt;/code&gt;
package. This flattens the hierarchical json file into one that is easier to work
with in R.&lt;/p&gt;

&lt;p&gt;I then start using the &lt;code class=&quot;highlighter-rouge&quot;&gt;tm&lt;/code&gt; package to put each item of ingredients into a Corpus, 
so we can start making transformations to the text and then create a Document 
Term Matrix that can be used in our various models.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;getTrainingData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jsonFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fromJSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simplifyDataFrame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ingredients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VectorSource&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ingredients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# future transformations to the ingredients will go here...
&lt;/span&gt;  
  &lt;span class=&quot;c1&quot;&gt;#  convert to document term matrix and coerce to data.frame
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DocumentTermMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ingredients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# add dependent variable (cuisine) from train object
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DV_cuisine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuisine&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# data.frame object returned
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;err&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getTrainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;train.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;distribution-of-cuisines&quot;&gt;Distribution of cuisines&lt;/h3&gt;

&lt;p&gt;There are twenty cuisines, as you can see below, the cuisines are not equally 
distributed. There are more Western cuisines than Eastern and quite a big 
proportion are Italian, Mexican, Indian, Southern US and Chinese.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/figure/treemap-1.png&quot; alt=&quot;plot of chunk treemap&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Below is a quick word cloud of the most common terms found in the ingredients lists. 
This gives us a bit of insight into how the data works, in our ‘bag of words’ structure
some of the words there don’t seem to make much sense without the preceding word 
(such as paste, powder, leaves etc) and others seem to be redundant adjectives (fresh,
large, chopped) which probably don’t tell us much about which cuisine the recipe might
belong to. I had a go at cleaning this data - and will explain hot this panned out in the next post.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;frq.Rda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;frq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wordcloud&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/figure/wordcloud-1.png&quot; alt=&quot;plot of chunk wordcloud&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;setting-up-a-partition&quot;&gt;Setting up a partition&lt;/h2&gt;

&lt;p&gt;In order to test our models, we need to separate our training data into a further
training and testing set. I did this using the caret package. This allows us to test our algorithms on a number of different training sets, altering the proportion of the 
training/testing sets if we wish.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;caret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;getPartitionForSeed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;set.seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;createDataPartition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DV_cuisine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Having done all of that, we’re ready to start coming up with a prediction model.&lt;/p&gt;

</description>
        <pubDate>Tue, 22 Dec 2015 00:00:00 +0000</pubDate>
        <link>/2015/12/22/whats-cooking-intro.html</link>
        <guid isPermaLink="true">/2015/12/22/whats-cooking-intro.html</guid>
        
        
      </item>
    
  </channel>
</rss>
